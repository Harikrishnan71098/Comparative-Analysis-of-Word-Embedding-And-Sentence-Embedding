# Comparative-Analysis-of-Word-Embedding-And-Sentence-Embedding
Using 20_news group data set and Fake news dataset.
Distributed word representation plays a pivotal role
in various natural language processing tasks. In spite
of its success, most existing methods only consider
contextual information, which is suboptimal when
used in various tasks due to a lack of task-specific
features. The rational word embeddings should have
the ability to capture both the semantic features and
task-specific features of words. I propose a taskoriented word embedding method and apply it to the
text classification task. With the function-aware
component, my method regularizes the distribution of
words to enable the embedding space to have a clear
classification boundary. I evaluate my method
using two text classification datasets. The experiment
results show that our method significantly
outperforms the state-of-the-art methods

Learning word representation is a fundamental step in
various natural language processing tasks.
Tremendous advances have been made by distributed
representations also known as word embeddings
which learn a transformation of each word from raw
text data to a dense, lower-dimensional vector space.
In traditional evaluations such as word similarity and
word analogy, the aforementioned context-aware
word embeddings work well since semantic
information plays a vital role in these tasks, and this
information is naturally addressed by word contexts.
However, in real-world applications, such as text
classification and information retrieval, word
contexts alone are insufficient to achieve success in
the absence of task-specific features. In this ,
I propose a task-oriented word embedding method
to solve the aforementioned problem. It learns the
distributed representation of words according to the
given specific
In this , I focus on two classification tasks,
which represent essential first filtering steps in the
process of extracting useful information. I
experiment with different types of word embeddings
and different types of supervised classifiers, with the
goal of gaining insights the embeddings that are most
suitable to use with crisis data and also into the
supervised models that result in better “generalized”
classifiers.
